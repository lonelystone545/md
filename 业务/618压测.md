[TOC]
# 压测目标
对于核心接口：分期计算页/收银台利益点文案
200并发，接口响应超时在100ms
下单：考拉2000TPS，下单占比10%，摸高15%，所以白条200-300TPS
查询：单机10(并发)*1000/100(rt)=100QPS(单机)

618时全站有20000QPS，一个接口最大的有5000QPS，下单有500TPS，全站平均RT有不到50ms，最慢有100ms。16台机器差不多单机有1000QPS。
# 计算公式
并发数/MRT = QPS/TPS
# 问题
1. 数据库连接池的minIdle未生效，导致在开始压测时，会创建新的连接，这里消耗了一定的时间；
2. 在计算可用活动时是多线程跑规则，线程池只有8个线程，为CPU核心数；（线程池写的有问题，局部变量创建线程池，性能并没有提升）
3. redis耗时太长，客户端监控耗时3-20ms占有一定的比例；
4. 云硬盘抖动，导致log.info耗时很久，出现大量rpc超时
5. dubbo log异步写
6. 运营活动以活动为角度，如果活动比较多，那么rt就会线性增长---解决：以商品维度来考虑;我们是小贷，目标是人群，所有是活动

# 排查和解决
## 数据库连接池问题
项目中采用的是druid连接池，在项目启动时，会创建已设置的初始化连接数=50，但是经过一段时间后，连接数下降，且小于设置的minIdle，当有大流量进来时，会创建连接直至50；
刚开始怀疑是配置问题或者是服务端关闭了连接，于是查看druid的官方文档，配置了keepAlive=true的参数，并将版本更新为当前的最新版本；并咨询DBA，mysql服务端是否配置多久关闭连接，DBA说没有；改完后重新测试，这次情况与上次不同了，是经过8小时后，连接数下降至minIdle以下；查阅资料发现，mysql默认会8小时关闭空闲的连接，这次比较明朗了，由于新版本的druid默认采用的是ping而非select检查网络连接有效性，因此增加了配置`spring.datasource.druid.valid-connection-checker-class-name=com.alibaba.druid.pool.ValidConnectionCheckerAdapter
`和`spring.datasource.druid.validation-query=select 1
`和`spring.datasource.druid.validation-query-timeout=${jdbc.validation-query-timeout:1000}
`，这样，客户端就会定期发送select 1给服务端，服务端会认为该链接一直被使用，而非空闲，就不会关闭了。
   tcp层面可能是一直在keepalive，但是mysql服务端在检测到这个连接没有数据时，会关闭连接（8小时）
<font color="#ff00">学会看源码，debug源码，入口：DruidDataSource</font>

## 多线程问题
在查看执行多线程的方法耗时时发现，一开始时方法耗时较短，但是大约几秒后，耗时逐渐增大。查看线程池的配置，发现核心线程数只有5，而多线程的执行逻辑主要就是，对多个活动跑多种规则，如商品规则，类名规则等，筛选出符合条件的活动，数据来源主要是redis缓存。这不是一个CPU密集型的任务，5个线程确实有点少。因此怀疑线程是核心线程个数太少，导致任务在队列中堆积，这样队列尾部的任务自然就处理很慢了。
一方面修改了线程池的大小为200，另一方面通过哨兵系统提供的功能，自定义采集器，定时采集线程池队列的大小。再次压测时，发现队列大小基本都小于100，平均RT提升了差不多50ms左右，没有最慢RT超过150ms的。
为什么是cpu+1？这个应该是一个经验值吧，即便是cpu密集型，cpu也不会一直在运转，毕竟cpu的处理速度很快，内存和cpu的速度也不是在一个数量级，出现了高速缓存。
## redis客户端监控3-20ms
redis服务端无慢查询日志slowlog(>3ms)。slowlog只是记录命令的执行时间，不包含排队等待、IO等待（入AOF SYNC）这类时间。而且观察redis连接池没有满。（当然，连接池未满，不能说明不存在阻塞，如果存在一个命令比较耗时，但是并发量没有那么大，那么连接数不会满的。连接池的连接数不是越大越好，会占用客户端和服务端的资源（维护更多的对象）；由压测决定，一个连接数的qps是5000，如果要求总qps为50000，那么需要10个连接）；查看redis的info信息，并未发现blocked clients的连接，然后加上网络监控，观察一天，发现网络情况并不稳定。说明是网络延迟原因，在pe和sa没办法解决的情况下，只能从业务代码进行优化。
    1. 加入本地缓存guavaCache；当并发请求一个失效的key时，只允许其中一个请求去查询数据库(回源，LoadingCache)，拿到返回值后塞进本地缓存，其他请求从本地缓存取值； ==> 避免缓存击穿，但是会导致其他请求阻塞等待结果(expireAfterAccess) ==> 为了提高吞吐量，可以采用refreshAfterWrite，这样当并发请求失效key时，第一个请求去查询数据库更新缓存值，其他请求返回旧值；（refreshAfter是只有在下次请求到达时才会触发刷新操作 ==> refresh带来的问题是，如果长时间未访问，则并发请求时可能会返回很久以前的数据，因此，需要将refresh和expire结合起来，这样能够保证数据未过期时再去执行refresh ==> 虽然refresh让其他请求直接返回旧值，但是第一个请求仍然同步回源加载数据，可以重写reload方法，异步获取数据。（参考guava部分）
    2. 系统读取redis缓存，不可避免会受网络抖动影响的，这里通过增加本地缓存减少网络IO。系统在初始化的时候会进行缓存预热(@PostConstruct，读取数据库，将每个活动对应的商品id读取到本地内存中，同时把dubbo改为延迟初始化，避免暴露服务过程中缓存还未加载完成)。从运营那边了解到，目前配置一个活动的商品数量最大不会超过50万，如果用int数组保存需要2M内存，目前最多同时存在100个活动，那么就是200M内存，如果活动数更多，那么占用的内存会更大，有的机器内存只有4g大小。因此这里采用了bitMap进行了适当的优化（bitmap可以节省空间资源，但是如果数组太过稀疏，那么可能采用bitmap占用的空间要比常规存储占用更大的空间）。这里有一些计算规则的，一个活动的最大商品ID-最小商品ID这个差值，会和活动中商品的数量进行比较。分析历史数据发现，活动中的商品的最大最小ID的差值小于5倍的数据量，这样的活动差不多占60%左右，那么针对这些活动的商品id存储会采用bitmap存储，将内存空间的占用降低到原来的1/8左右。常规的存储采用HashSet结构，java中的BitSet提供了这种数据结构。通过ConcurrentHashMap存储，key为活动的id，value为BitSet。为了防止BitSet扩容(*2)引起的内存翻倍，初始化时指定了长度为50万(消耗内存很少，60K左右)，指定了ConcurrentHashMap的初始化大小为200。本地缓存更新：第一是在系统初始化时会从数据库中加载数据，进行缓存预热；第二是当配置新的活动时，会通过NDC(cannal)订阅binlog日志变更，从kafka中读取消息，刷分布式缓存redis，然后通过MQ分发到不同机器做刷本地缓存的动作(kafka也可以做到，不主动分配groupId，那么会自动创建id)。同时也预留了刷本地缓存的接口。redis中采用Set集合中存储商品id等，这是一个大set，所以我们在写到redis时是分批写的，sadd每次增加1000个这样来做，避免阻塞redis单线程。 删除时也是利用scan命令分批删除的。但是在redis4.0支持的lazyfree异步删除机制。
怎么解决稀疏数组问题？从运营那边了解到，基本上是商品id的范围比较靠近会在一个活动中，所以用最大商品id-最小商品id来决定位数组的大小，但是依然可能存在稀疏数组的问题。可以采用压缩算法，如WAH和Roaring等，WAH的思路如下：将bitmap按照31位进行分组，每个分组为一个block，如果block包含0和1，则block以1+block的内容表示，如果只包含0，则以00+N表示；如果block只包含1，则以01+N表示；N表示block的数量；也就是说对于连续的0或者连续的1进行压缩了，这样在查询的时候，首先先定位属于哪个block，num/31；然后在一个block中判断即可，对31取模，%31。
如果数据很多，bitSet放不下如何解决？可以采用多个BitSet，将数据分段存储在不同的BitSet中；
    
## log异步写日志
参考 日志-logback异步 章节

## 模型调整
问题：随着活动数量增加，性能呈线性变差，比如，现在常用的规则是商品和类目规则，线上配置了100个活动，那么每个用户访问时，需要对100个活动分别进行两次规则的计算，随着活动变多，规则计算会越来越多，无法优化。=== 主要是因为，我们白条是针对用户做活动，而不是类似电商一样，是根据商品做活动。
优化：改成根据商品做活动，从历史数据统计来看，大多数用户一次性购买的商品数量差不多在2件，呈正态分布。如果以商品做活动，那么需要预先拿到每个商品满足的活动，然后做交集，取出公共的活动，一个商品参与的活动不会有几十个，那么就限制住了规则计算前的活动个数。相当于在之前的规则计算前，提前做了一层过滤，将商品和活动的关系提前保存下来，而不是通过规则计算完成，减少计算量。时间复杂度为O(N)，如果N很大，确实也是有问题的。咨询过运营，一般来说一个商品参与的活动也就3-4个，所以不是问题的。